{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple dictionary preparation for every language that is supported by Google Translate API\n",
    "Provide file with 50k words from: https://github.com/hermitdave/FrequencyWords (or in corresponding format) and go to [Running code](#running-code) section to create own dictionary"
   ],
   "id": "10a7a5a72a944108"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports & config",
   "id": "6d2e5a5bad6af0a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:17.338148Z",
     "start_time": "2025-05-22T17:03:16.891325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from googletrans import Translator\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, OrderedDict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\") # run manually: python -m spacy download pl_core_news_sm\n",
    "\n",
    "def ensure_nltk_resource(resource, download_name=None):\n",
    "    try:\n",
    "        nltk.data.find(resource)\n",
    "    except LookupError:\n",
    "        nltk.download(download_name or resource.split('/')[-1])\n",
    "\n",
    "ensure_nltk_resource('corpora/wordnet')\n",
    "ensure_nltk_resource('corpora/omw-1.4', 'omw-1.4')"
   ],
   "id": "66b1e3a6e4966fa9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/grzegorzpozorski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/grzegorzpozorski/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:17.368238Z",
     "start_time": "2025-05-22T17:03:17.359790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator_batch_size = 32\n",
    "semaphore_max_concurrency = 48 # to control the number of concurrent requests to Google Translate API\n",
    "translator_list_operation_max_concurrency = 32\n",
    "\n",
    "translator = Translator(service_urls=['translate.googleapis.com'], list_operation_max_concurrency=translator_list_operation_max_concurrency)"
   ],
   "id": "61ead45a9d2e2a58",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions",
   "id": "63d5dd43f4d191c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions for reading and preprocessing the input file",
   "id": "172e6fbc4183628c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:06:29.364144Z",
     "start_time": "2025-05-22T17:06:29.313157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_file_to_memory(input_path) -> List[Tuple[str, int]]:\n",
    "    # we expect to process files with 50k lines\n",
    "    with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "        input_file.seek(0)\n",
    "        lines = [line.strip() for line in input_file]\n",
    "        read_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            word, occurrence = parts\n",
    "            read_lines.append((word, occurrence))\n",
    "\n",
    "    return read_lines\n",
    "\n",
    "\n",
    "def deduplicate_lines(lines_to_process):\n",
    "    unique = OrderedDict()\n",
    "    for lemma, occurrence in lines_to_process:\n",
    "        if lemma not in unique:\n",
    "            unique[lemma] = occurrence\n",
    "    return list(unique.items())\n",
    "\n",
    "\n",
    "def get_valid_lemmatized_nouns(words_with_occurrences, minimal_word_length=4, maximal_word_length=8) -> List[Tuple[str, int]]:\n",
    "    batch_size = 64\n",
    "    valid_nouns_with_occurrences = []\n",
    "\n",
    "    docs = nlp.pipe([word for word, _ in words_with_occurrences], batch_size=batch_size)\n",
    "    for (word, occurrence), doc in tqdm(zip(words_with_occurrences, docs), total=len(words_with_occurrences), desc=\"Processing lines\"):\n",
    "        lemma = doc[0].lemma_\n",
    "        if minimal_word_length <= len(lemma) <= maximal_word_length:\n",
    "            if doc[0].pos_ == \"NOUN\":\n",
    "                valid_nouns_with_occurrences.append((lemma, occurrence))\n",
    "\n",
    "    print(f\"Valid lemmatized nouns count: {len(valid_nouns_with_occurrences)}\")\n",
    "    deduplicated_valid_nouns_with_occurrences = deduplicate_lines(valid_nouns_with_occurrences)\n",
    "    print(f\"Deduplicated, valid, lemmatized nouns count: {len(deduplicated_valid_nouns_with_occurrences)}\")\n",
    "\n",
    "    return deduplicated_valid_nouns_with_occurrences\n"
   ],
   "id": "ae05ee6527eea39f",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions for categorizing nouns",
   "id": "470d05ec135f4a50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:06:33.528567Z",
     "start_time": "2025-05-22T17:06:33.480283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_english_noun_category(english_noun):\n",
    "    synsets = wordnet.synsets(english_noun, pos=wordnet.NOUN)\n",
    "    if not synsets:\n",
    "        return \"unknown\"\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    if hypernyms:\n",
    "        return hypernyms[0].lemma_names()[0]\n",
    "    return \"general_noun\"\n",
    "\n",
    "\n",
    "async def categorize_nouns_batch(batch, semaphore, original_dictionary_language):\n",
    "    language_for_categorization = 'en'\n",
    "\n",
    "    async with semaphore:\n",
    "        lemmas = [lemma for lemma, _ in batch]\n",
    "        english_translations = await translator.translate(lemmas, src=original_dictionary_language, dest=language_for_categorization)\n",
    "        \n",
    "        english_words = [t.text.lower() for t in english_translations]\n",
    "        categories = [get_english_noun_category(w) for w in english_words]\n",
    "        \n",
    "        valid_indices = [idx for idx, cat in enumerate(categories) if cat not in (\"unknown\", \"general_noun\")]\n",
    "        \n",
    "        results = []\n",
    "        mismatch_count = 0\n",
    "        unknown_general_category_count = 0\n",
    "        \n",
    "        if not valid_indices:\n",
    "            unknown_general_category_count += len(batch)\n",
    "            return results, mismatch_count, unknown_general_category_count\n",
    "        \n",
    "        valid_english_words = [english_words[idx] for idx in valid_indices]\n",
    "\n",
    "        original_language_translations = await translator.translate(valid_english_words, src=language_for_categorization, dest=original_dictionary_language)\n",
    "        original_language_translations = [t.text.lower() for t in original_language_translations]\n",
    "\n",
    "        for j, idx in enumerate(valid_indices):\n",
    "            lemma, occurrence = batch[idx]\n",
    "            if original_language_translations[j] != lemma:\n",
    "                mismatch_count += 1\n",
    "                continue\n",
    "\n",
    "            results.append(f\"{lemma} {occurrence} {categories[idx]}\\n\")\n",
    "\n",
    "        return results, mismatch_count, unknown_general_category_count\n",
    "\n",
    "\n",
    "async def categorize_nouns(lines_to_process, output_path, original_dictionary_language):\n",
    "    semaphore = asyncio.Semaphore(semaphore_max_concurrency)\n",
    "    batches = [lines_to_process[i:i+translator_batch_size] for i in range(0, len(lines_to_process), translator_batch_size)]\n",
    "    tasks = [categorize_nouns_batch(batch, semaphore, original_dictionary_language) for batch in batches]\n",
    "\n",
    "    mismatch_count = 0\n",
    "    unknown_general_category_count = 0\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Processing batches\"):\n",
    "            results, mismatches, unknowns = await fut\n",
    "            mismatch_count += mismatches\n",
    "            unknown_general_category_count += unknowns\n",
    "            outfile.writelines(results)\n",
    "\n",
    "    print(f\"Processed: {mismatch_count} mismatches, {unknown_general_category_count} unknown/general categories\")"
   ],
   "id": "24c7dd3ca645aa53",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions for checking categories",
   "id": "2d4c4436152e8680"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:06:36.207940Z",
     "start_time": "2025-05-22T17:06:36.198817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_by_category(input_path):\n",
    "    category_dict = {}\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            word, count, category = parts[0], parts[1], parts[2]\n",
    "            if category not in category_dict:\n",
    "                category_dict[category] = []\n",
    "            category_dict[category].append((word, count))\n",
    "\n",
    "    return category_dict\n",
    "\n",
    "def get_top_categories(category_dict, min_words=10, should_print=True):\n",
    "    ordered_category_dict = dict(sorted(category_dict.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "    top_categories_dict = {}\n",
    "    cnt = 0\n",
    "    for category, words in ordered_category_dict.items():\n",
    "        if len(words) < min_words:\n",
    "            continue\n",
    "        if should_print:\n",
    "            print(f\"Category: {category}, Words count: {len(words)}\")\n",
    "        top_categories_dict[category] = words\n",
    "        cnt += 1\n",
    "\n",
    "    if should_print:\n",
    "        print(f\"\\nCategories with {min_words}+ words count: {cnt}\")\n",
    "\n",
    "    return top_categories_dict\n",
    "\n",
    "\n",
    "def get_nouns_lengths_sorted(input_path):\n",
    "    counts = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            word, count, category = parts[0], parts[1], parts[2]\n",
    "            counts.append(len(word))\n",
    "\n",
    "\n",
    "    counter = Counter(counts)\n",
    "    sorted_counter = dict(sorted(counter.items(), key=lambda item: item[0]))\n",
    "\n",
    "    return sorted_counter\n",
    "\n",
    "\n",
    "async def save_top_categories(top_categories_dict, original_dictionary_language, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for category, entry in top_categories_dict.items():\n",
    "            if \"_\" in category or \"-\" in category:\n",
    "                category = category.replace(\"_\", \" \")\n",
    "            category_translation = await translator.translate(category, src='en', dest=original_dictionary_language)\n",
    "            original_language_category = category_translation.text.lower()\n",
    "            for word, count in entry:\n",
    "                outfile.write(f\"{word} {count} {original_language_category}\\n\")"
   ],
   "id": "8c0b54d5449c07e4",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running code",
   "id": "f1a6cf31e5f99c6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:14:50.379147Z",
     "start_time": "2025-05-22T17:14:16.404068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_dictionary_language = 'pl'\n",
    "directory = f\"data/{original_dictionary_language}\"\n",
    "\n",
    "input_path = f\"{directory}/{original_dictionary_language}_50k.txt\"\n",
    "assert os.path.exists(input_path), f\"File {input_path} does not exist. Please download the file from: https://github.com/hermitdave/FrequencyWords\"\n",
    "\n",
    "output_path_categorized_dict = f\"{directory}/{original_dictionary_language}_output_categorized.txt\"\n",
    "output_path_top_categories = f\"{directory}/{original_dictionary_language}_top_categories.txt\"\n",
    "\n",
    "words_with_occurrences = read_file_to_memory(input_path)\n",
    "nouns_with_occurrences = get_valid_lemmatized_nouns(words_with_occurrences)\n",
    "# processing time: ~36s"
   ],
   "id": "ab6d13ea8ac33e91",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 50000/50000 [00:33<00:00, 1475.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid lemmatized nouns count: 17067\n",
      "Deduplicated, valid, lemmatized nouns count: 10606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:15:36.950138Z",
     "start_time": "2025-05-22T17:14:50.394186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asyncio.run(categorize_nouns(nouns_with_occurrences, output_path_categorized_dict, original_dictionary_language))\n",
    "# processing time for 13k records: ~60s"
   ],
   "id": "5dc12359780fc355",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 332/332 [00:46<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 4563 mismatches, 0 unknown/general categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:15:37.768807Z",
     "start_time": "2025-05-22T17:15:36.961542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "category_dict = group_by_category(output_path_categorized_dict)\n",
    "top_categories_dict = get_top_categories(category_dict, min_words=10, should_print=True)\n",
    "asyncio.run(save_top_categories(top_categories_dict, original_dictionary_language, output_path_top_categories))"
   ],
   "id": "2f7464f4a6b22ba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: person, Words count: 25\n",
      "Category: activity, Words count: 14\n",
      "Category: structure, Words count: 13\n",
      "Category: container, Words count: 11\n",
      "Category: property, Words count: 11\n",
      "Category: dish, Words count: 11\n",
      "Category: Gregorian_calendar_month, Words count: 10\n",
      "Category: woman, Words count: 10\n",
      "Category: state, Words count: 10\n",
      "Category: area, Words count: 10\n",
      "\n",
      "Categories with 10+ words count: 10\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:15:37.788322Z",
     "start_time": "2025-05-22T17:15:37.783097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counter = get_nouns_lengths_sorted(output_path_categorized_dict)\n",
    "print(\"Counts of words by length:\")\n",
    "for length, count in counter.items():\n",
    "    print(f\"Length {length}: {count} words\")"
   ],
   "id": "2351b114c67b13da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of words by length:\n",
      "Length 4: 339 words\n",
      "Length 5: 569 words\n",
      "Length 6: 584 words\n",
      "Length 7: 546 words\n",
      "Length 8: 463 words\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notes\n",
    "- concurrency equal 48 is set after several attempts to find the best value\n",
    "- we can have more words in dict, if we classify words into multiple categories (i.e. 'nail' can be 'nail' and 'finger_nail')\n",
    "- there is not specific error handling"
   ],
   "id": "195afebee9ccf982"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:55.724272Z",
     "start_time": "2025-05-22T17:03:55.722631Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4ab91145d201ad64",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
